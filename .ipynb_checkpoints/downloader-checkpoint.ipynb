{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "A downloader that catch the data from Chinese land data website.\n",
    "\n",
    "Yuhao Zhu\n",
    "\n",
    "20170224: Creation of the file and adjust the code.\n",
    "20170225: Create a log file for links that are not able to connect.\n",
    "\"\"\"\n",
    "\n",
    "import requests, re, os, sys, csv, datetime\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Downloader():\n",
    "    def __init__(self):\n",
    "        self.url='http://www.landchina.com/default.aspx?tabid=263'\n",
    "        # 2011年后的出让公告\n",
    "        self.post_data={'TAB_QueryConditionItem':'9f2c3acd-0256-4da2-a659-6949c4671a2a',\n",
    "                        'TAB_QuerySortItemList':'282:False',\n",
    "                        # date\n",
    "                        'TAB_QuerySubmitConditionData':'9f2c3acd-0256-4da2-a659-6949c4671a2a:',  \n",
    "                        'TAB_QuerySubmitOrderData':'282:False',\n",
    "                        # page number\n",
    "                        'TAB_QuerySubmitPagerData':''} \n",
    "        self.row_name=[u'行政区', u'电子监管号', u'项目名称', u'项目位置', u'面积(公顷)', u'土地来源',\n",
    "                      u'土地用途', u'供地方式', u'土地使用年限', u'行业分类', u'土地级别', u'成交价格(万元)',\n",
    "                      u'土地使用权人', u'约定容积率下限', u'约定容积率上限', u'约定交地时间', u'约定开工时间',\n",
    "                      u'约定竣工时间', u'实际开工时间', u'实际竣工时间', u'批准单位', u'合同签订日期']\n",
    "        # 要抓取的数据名称。除了\"分期约定\"的四项以外全部抓取\n",
    "        self.information=[\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r1_c2_ctrl',#0\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r1_c4_ctrl',#1\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r17_c2_ctrl',#2\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r16_c2_ctrl',#3\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r2_c2_ctrl',#4\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r2_c4_ctrl',#5\n",
    "            # 此条为土地来源，抓取为数字，经过换算方得土地来源。\n",
    "            # 如果‘土地来源’=0 则为’新增建设用地‘\n",
    "            # 如果’土地来源=‘土地用途’ 则为‘现有建设用地’\n",
    "            # 如果其他 则为‘新增建设用地（来自存量库）’。\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r3_c2_ctrl',#6  \n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r3_c4_ctrl',#7\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r19_c2_ctrl', #8              \n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r19_c4_ctrl',#9\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r20_c2_ctrl',#10\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r20_c4_ctrl',#11\n",
    "##          'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f3_r2_c1_0_ctrl',\n",
    "##          'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f3_r2_c2_0_ctrl',\n",
    "##          'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f3_r2_c3_0_ctrl',\n",
    "##          'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f3_r2_c4_0_ctrl',\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r9_c2_ctrl',#12\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f2_r1_c2_ctrl',\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f2_r1_c4_ctrl',\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r21_c4_ctrl',\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r22_c2',\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r22_c4_ctrl',\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r10_c2_ctrl',\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r10_c4_ctrl',                \n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r14_c2_ctrl',\n",
    "            'mainModuleContainer_1855_1856_ctl00_ctl00_p1_f1_r14_c4_ctrl']\n",
    "\n",
    "# Step 1\n",
    "    def date_format(self,year,month,day):\n",
    "        \"\"\"\n",
    "        Import year, month and day, and return date format %Y-%m-%d\n",
    "        \"\"\"\n",
    "        date = datetime.date(year, month, day)\n",
    "        return date\n",
    "    \n",
    "    def days_in_month(self,year,month):\n",
    "        \"\"\"\n",
    "        Calculate how many days in a specific month.\n",
    "        \"\"\"\n",
    "        date = datetime.date(year, month, 1) # The date of the first day of a certain month.\n",
    "        try:    \n",
    "            date_next = datetime.date(date.year, date.month + 1, date.day) # The date of the first day of next month.\n",
    "        except:\n",
    "            date_next = datetime.date(date.year + 1, 1, date.day)  \n",
    "        date_distance = (date_next - date).days # How many days in between.\n",
    "        return date_distance\n",
    "    \n",
    "    def get_page_content(self, page_number, date):\n",
    "        \"\"\"\n",
    "        Specify the date to be searched, open the first corresponding page, get the contents.\n",
    "        \"\"\"\n",
    "        post_data = self.post_data.copy()\n",
    "        # set up the dates to be searched.\n",
    "        query_date = date.strftime('%Y-%m-%d') + '~' + date.strftime('%Y-%m-%d')\n",
    "        post_data['TAB_QuerySubmitConditionData'] += query_date\n",
    "        # set up the page\n",
    "        post_data['TAB_QuerySubmitPagerData'] = str(page_number)\n",
    "        try:\n",
    "        # request the url\n",
    "            r = requests.post(self.url, data=post_data, timeout=60)\n",
    "            r.encoding='gb18030'\n",
    "            page_content = r.text\n",
    "        except:\n",
    "            print('Loading page {} timeout!'.format(self.url))\n",
    "            page_content = u'没有检索到相关数据'\n",
    "        return page_content\n",
    "\n",
    "# Step 2\n",
    "    def get_all_number(self, date):\n",
    "        \"\"\"\n",
    "        Find the number of pages for the date being searched.\n",
    "        Case 1: no content\n",
    "        Case 2: only one page of result\n",
    "        Case 3: 1 to 200 pages\n",
    "        Case 4: 200 pages above.\n",
    "        \"\"\"  \n",
    "        first_content = self.get_page_content(1, date)\n",
    "        if u'没有检索到相关数据' in first_content:\n",
    "            print('Date {} have 0 page.'.format(date))\n",
    "            return 0\n",
    "        pattern = re.compile(u'<td.*?class=\"pager\".*?>共(.*?)页.*?</td>')\n",
    "        result = re.search(pattern, first_content)\n",
    "        if result == None:\n",
    "            print('Date {} have 1 page.'.format(date))\n",
    "            return 1\n",
    "        if int(result.group(1)) <= 200:\n",
    "            print('Date {0} have {1} pages.'.format(date, int(result.group(1))))\n",
    "            return int(result.group(1))\n",
    "        else:\n",
    "            print('Date {} have more than 200 pages. Only first 200 pages will be downloaded!'.format(date))\n",
    "            return 200\n",
    "        \n",
    "# Step 3\n",
    "    def get_links(self, page_number, date):\n",
    "        \"\"\"get links from a page\"\"\"\n",
    "        page_content = self.get_page_content(page_number, date)\n",
    "        links = []\n",
    "        pattern = re.compile(u'<a.*?href=\"default.aspx.*?tabid=386(.*?)\".*?>', re.S)\n",
    "        results = re.findall(pattern, page_content)\n",
    "        for result in results:\n",
    "            links.append('http://www.landchina.com/default.aspx?tabid=386' + result)\n",
    "        return links\n",
    "    \n",
    "    def get_all_links(self, all_number, date):\n",
    "        \"\"\"get all links\"\"\"\n",
    "        page_number = 1\n",
    "        all_links = []\n",
    "        while page_number <= all_number:\n",
    "            links = self.get_links(page_number, date)\n",
    "            all_links += links\n",
    "            print('Get links from the page {0} out of {1}.'.format(page_number, all_number))\n",
    "            page_number += 1\n",
    "        print('Date {0} have {1} links.'.format(date, len(all_links)))\n",
    "        return all_links\n",
    "    \n",
    "# Step 4\n",
    "    def get_link_content(self, link):\n",
    "        \"\"\"\n",
    "        Open the link to get the content of that link. The content is the report.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            r=requests.get(link, timeout=60)\n",
    "            r.encoding='gb18030'\n",
    "            link_content = r.text\n",
    "        except:\n",
    "            print('Loading report from {} timeout!'.format(link))\n",
    "            link_content = \"\"\n",
    "            error_file_name = 'landchina/error.csv'\n",
    "            if os.path.exists(error_file_name):\n",
    "                mode = 'a'\n",
    "            else:\n",
    "                mode = 'w'\n",
    "            csv_file = open(error_file_name, mode, encoding='utf-8-sig')\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow(['{}'.format(link)])\n",
    "            csv_file.close()\n",
    "        return link_content\n",
    "    \n",
    "    def get_information(self,link_content):\n",
    "        \"\"\"\n",
    "        Get information from every item.\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        soup = BeautifulSoup(link_content, 'lxml')\n",
    "        for item in self.information:\n",
    "            if soup.find(id=item) == None:\n",
    "                s = ''\n",
    "            else:\n",
    "                s = soup.find(id=item).string\n",
    "                if s == None:\n",
    "                    s=''                \n",
    "            data.append(s.strip())\n",
    "        return data\n",
    "    \n",
    "    def save_information(self, data, date):\n",
    "        \"\"\"\n",
    "        Save the information to the file.\n",
    "        \"\"\"\n",
    "#         path is \"landchina/year/month/day.csv\"\n",
    "        file_name = 'landchina/{0}/{1}/{2}.csv'.format(datetime.datetime.strftime(date,'%Y'),\n",
    "                                                      datetime.datetime.strftime(date,'%m'),\n",
    "                                                      datetime.datetime.strftime(date,'%d'))\n",
    "        if os.path.exists(file_name):\n",
    "            mode = 'a'\n",
    "        else:\n",
    "            mode = 'w'\n",
    "        csv_file = open(file_name, mode, encoding='utf-8-sig')\n",
    "        writer = csv.writer(csv_file)\n",
    "        if mode == 'w':\n",
    "            writer.writerow([name for name in self.row_name])\n",
    "        writer.writerow([d for d in data])\n",
    "        csv_file.close()\n",
    "        \n",
    "    def make_directory(self, date):\n",
    "        \"\"\"\n",
    "        Create the directory.\n",
    "        \"\"\"\n",
    "        path = 'landchina/{0}/{1}'.format(datetime.datetime.strftime(date,'%Y'),\n",
    "                                          datetime.datetime.strftime(date,'%m'))\n",
    "        isExists = os.path.exists(path)\n",
    "        if not isExists:\n",
    "            os.makedirs(path)\n",
    "            \n",
    "    def save_all_information(self, all_links, date):\n",
    "        \"\"\"\n",
    "        Save all information from the website.\n",
    "        \"\"\"\n",
    "        for (i,link) in enumerate(all_links):\n",
    "            link_content = data = None\n",
    "            link_content = self.get_link_content(link)\n",
    "            data = self.get_information(link_content)\n",
    "            self.make_directory(date)\n",
    "            self.save_information(data, date)\n",
    "            print('Save information from link {0} out of {1}.'.format(i+1, len(all_links)))\n",
    "        \n",
    "    def land_download(self, year, month, day):\n",
    "        days = self.days_in_month(year, month)\n",
    "        print('Start downloading!\\n')\n",
    "        # Download month by month\n",
    "        while day <= days:\n",
    "            # date\n",
    "            date = self.date_format(year, month, day)\n",
    "            # page\n",
    "            all_number = self.get_all_number(date)\n",
    "            # link\n",
    "            all_links = self.get_all_links(all_number, date)\n",
    "            # information\n",
    "            self.save_all_information(all_links, date)\n",
    "            day += 1\n",
    "            print('The information for date {} is all downloaded!\\n'.format(date))\n",
    "        print('The information for month {} is all downloaded!'.format(date.strftime('%Y-%m')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start downloading!\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'machine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-08f8e97622b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mland_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2013\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# # Date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# year = 2013\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-53be964d857d>\u001b[0m in \u001b[0;36mland_download\u001b[0;34m(self, year, month, day)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mday\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mdays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;31m# date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;31m# page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mall_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'machine' is not defined"
     ]
    }
   ],
   "source": [
    "spider = Downloader()\n",
    "spider.land_download(2013, 7, 1)\n",
    "\n",
    "# # Date\n",
    "# year = 2013\n",
    "# month = 6\n",
    "# day = 1\n",
    "# days = machine.days_in_month(year, month)\n",
    "\n",
    "# print('Start downloading!\\n')\n",
    "# # Download month by month\n",
    "# while day <= days:\n",
    "#     # date\n",
    "#     date = machine.date_format(year, month, day)\n",
    "#     # page\n",
    "#     all_number = machine.get_all_number(date)\n",
    "#     # link\n",
    "#     all_links = machine.get_all_links(all_number, date)\n",
    "#     # information\n",
    "#     machine.save_all_information(all_links, date)\n",
    "#     day += 1\n",
    "#     print('The information for date {} is all downloaded!\\n'.format(date))\n",
    "# print('The information for month {} is all downloaded!'.format(date.strftime('%Y-%m')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
